# CalculusAndMathFunctions
Work in progress as said. Includes numerical calculus functions and operators like derivatives, integrals and partial derivatives of one or more variables and operators for vector functions. Multiple integrals for functions of many variables (loses precision with iteration), extremes of integration in form of function (not working properly), jacobian and differential operators. Includes basic operations with vectors and matrices like scalar product, product by number, cross product and NXN determinant calculator. Also useful elementary functions (trigonometric, etc.), irrational constants calculator (pi and golden ratio) and Fourier series numerical approximation for any function. I also plan to include differential equation numerical solvers and non-elementary functions in the future.

Still working on multiple incomplete integrals (in form of function) and integrals with extremes in form of function, but there's something wrong with the way the code reads the inputted integration order versus the order it actually integrates and thus end up mistaking a variable for another. This is a minor problem, actually, that I can solve alone but haven't had the time because of a college study in physics. In fact, my idea when I began making this (~2 to 3 years ago) was for personal use to help me verify my answers in calculus and physics because I hadn't access to software like WOLFRAM's Mathematica at the time.
Also worth noting that they are not based off fast theories/algorithms that were already developed by mathematicians to calculate these values numerically, but they were made upon basic calculus concepts and definitions, because I don't possess any knowledge of said algorithms or core python programming knowledge. So my capabilities are very limited and one who's interested in a more complex approach toward physics simulation, for instance, would be better served with a good official library that's out there, with powerful software.

The variable "epsilon" describes the inferior limit below which it considers that the function converged. It's made by the Cauchy Sequence convergence statement applied to the last two values, measuring at each iteration the absolute value of the difference of those two*.
*Every calculus operation is made discretely because of the way the computer works, so we're talking about sequences for limits, be they derivatives or integrals. This of course, requires the hypothesis that said function converges when x tends to the point in the real line equal to the limit of that sequence, so the code isn't useful for one who isn't sure if it really converges because it uses an arbitrary sequence for calculating that limit. So if it diverges, you will simply get an infinite loop. And if that limit exists, then you'll get its value, for the precision given. Also, when the limit is a non-periodic rational number you won't get the exact answer and perhaps it's useful to round the answer to truncate the value that's beyond the given precision. So, for example, if the limit is an integer "2", the code won't round the answer, so you'll get something like "2.000000003" or "1.999999997".

While calculus.py can provide derivatives of order greater than 3, I don't recommend it. It does that by recursion but it's not reliable because the error gets greater with each iteration and there's a big precision loss. That also happens with integrals and limits.
You might think "why not shrink that epsilon value"
But sometimes the problem is with the algorithm and not its parameters, because it doesn't do that in a specially fast way, but by the definition of integrals and derivatives. So and seeing as the computer only deals with floating-point numbers there's an extent to which it can preserve the error, putting it simply, besides which there's a loss of information due to limited number of digits in memory. Not to mention that the process is increasingly taxating to the cpu with each iteration and demands more and more time with the decrease of the "epsilon" value, a time interval which increases exponentially.
So if you want to get a numerical answer to the integral of a complicated expression in a reasonable amount of time, you should increase the epsilon to ten times the initial value and go from that. When dealing with a double integral you (disappointedly) should choose a precision of 0.1 for getting it within a minute or two. Still, if you can't use an online software or something elaborated like Mathematica, numerical integration via Python provides a good idea for an answer to a problem that you're not sure of. Of course, if it's a long expression that takes 30 seconds to calculate you might not want to turn it into a graph, so still not your ideal analitical problem-solver. But rest assured, it didn't let me down for some vector calculus answers I needed to verify, though some of them are not even possible to get even at the precision of 0.1, still requiring an effort to change variables to another system of coordinates and manipulating/breaking it into simpler parts.

You can use it to get numerical answers that cannot be expressed analitically with elementary functions, like with the gaussian curve, giving you that sweet number you're looking for when you know the distribution without having to memorise the probability of a boltzmann distribution being within a certain multiple of the standard deviation, for example.

I still am looking for writing things like power series for Bessel Functions and other special orthogonal polynomials so you could make a graph, but still haven't studied enough of the subject. I'll try in the future to adapt the vector analysis to a complex plane analysis, thus calculating numerically fourier transforms, complex derivatives and integrals.

My python programming knowledge goes more toward calculating algorithms and automating math operations, not complex stuff that messes with huge matrices and strings such as the ones you'd probably see in a basic videogame script, but still is probably useful for physics simulation and stuff like that.
So I'm pretty much of a noob in python in many ways and the things I learned were either from the web or my algorithms introduction class in Physics college, but mostly the algorithms class. And then there's the human value in the form of insights into how to make the theory work from paper to computer instruction and ideas of how to do things I needed for the function, for which sometimes it took a great lot of effort and time for creativity to do its thing and implementation, adaptation or even rethinking the whole thing after testing it by trial and error. In spite of artificial intelligence advances these days, it seems humanity still cannot be replaced for some things (for now), and that's a great relief for someone whose field of study is in the exact sciences, but you probably didn't ask for my take on the matter.

As said before, it still was developed originally for personal use, so the names of the functions and descriptions of what they do have not yet been translated from brazillian portuguese, but I believe the way they were supposed to be used and what they do are not hard to guess and find out for a programmer, by looking at the code and function names, for example. You still need something like google translate for the descriptions though, but the important part is written in mathematical notation, save for explanations of how it does things, but that does not mean I won't be making a guide in english explaining what each function does and how its syntax works in the future.

And, yes, I'm doing it mostly for fun and finding a use for the theorems I learned, many of which can be considered early algorithms. It sure's no secret that the finding or creation of algorithms was present in math studies much earlier than the time when the first computer (as we know it) was made. And even before Ada Lovelace they were developed (unofficially) perhaps at least partially in the form of theorems, like Laplace's theorem for calculating determinants. So the computer implementation of all that stuff kind of proceeds naturally from that. You could even say that math theory until now has been a kind of algorithm, with the term "algorithm" being very broadly used in that regard. 

Future projects for the library include Fourier Transforms, Special Functions, differential equation infinite series solver and complex analysis calculators, and maybe internal product between functions and other linear algebra that uses calculus.
Also in the matrices_and_vectors.py I'll think about some way of automatizing the Gram-Schmidt process and maybe extend it to functions with the calculus module to develop polynomial approximation coefficients for functions, and the adjustment of curves with the minimum-square method.

Also, in matrices_and_vectors.py there's a matrix inverter there, and the inversion method of solving possible and determined sets of linear equations. That could be useful when changing referential systems that are spinning or moving in a certain way, and convert the angles quickly for the use in 3D perspective programs, though I'm not sure exactly how it could be done. Also the solving of those Khan-Academy aX+B=C problems that I made out of boredom to cheat in an online linear algebra homework for college, where "a" is a scalar value, and yes, you could again use it to change variables in a moving referential galileo transform by moving the vector and de-spinning it with an inverse matrix or the other way around.
